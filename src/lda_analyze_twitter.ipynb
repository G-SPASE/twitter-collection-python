{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# CORPUS_PATH = os.path.join('data/', 'ClassifiedTweet_sample')\n",
    "CORPUS_PATH = os.path.join('/Users/koitaroh/Documents/Data/Tweet/', 'ClassifiedTweet_20150709')\n",
    "filenames = sorted([os.path.join(CORPUS_PATH, fn) for fn in os.listdir(CORPUS_PATH)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1484214"
      ]
     },
     "execution_count": 2,
     "output_type": "execute_result",
     "metadata": {}
    }
   ],
   "source": [
    "len(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools, operator, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grouper(n, iterable, fillvalue=None):\n",
    "    \"Collect data into fixed-length chunks or blocks\"\n",
    "    args = [iter(iterable)] * n\n",
    "    return itertools.zip_longest(*args, fillvalue=fillvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "doctopic_triples = []\n",
    "mallet_docnames = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"/Users/koitaroh/Documents/GitHub/GeoTweetCollector/data/doc-topics-tweet.txt\", encoding=\"utf-8\") as f:\n",
    "with open(\"/Users/koitaroh/Documents/GitHub/GeoTweetCollector/data/doc-topics-tweet-20150709.txt\", encoding=\"utf-8\") as f:\n",
    "    f.readline()  # read one line in order to skip the header\n",
    "    \n",
    "    for line in f:\n",
    "        docnum, docname, *values = line.rstrip().split('\\t')\n",
    "        \n",
    "        mallet_docnames.append(docname)\n",
    "        for topic, share in grouper(2, values):\n",
    "            triple = (docname, int(topic), float(share))\n",
    "            doctopic_triples.append(triple)\n",
    "            \n",
    "# sort the triples\n",
    "# triple is (docname, topicnum, share) so sort(key=operator.itemgetter(0,1))\n",
    "# sorts on (docname, topicnum) which is what we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('file:/Users/koitaroh/Documents/Data/Tweet/ClassifiedTweet_20150709/2015031714_12147_3122.txt',\n",
       "  0,\n",
       "  0.03676470588235294),\n",
       " ('file:/Users/koitaroh/Documents/Data/Tweet/ClassifiedTweet_20150709/2015031714_12147_3122.txt',\n",
       "  1,\n",
       "  0.03676470588235294),\n",
       " ('file:/Users/koitaroh/Documents/Data/Tweet/ClassifiedTweet_20150709/2015031714_12147_3122.txt',\n",
       "  2,\n",
       "  0.0661764705882353),\n",
       " ('file:/Users/koitaroh/Documents/Data/Tweet/ClassifiedTweet_20150709/2015031714_12147_3122.txt',\n",
       "  3,\n",
       "  0.03676470588235294),\n",
       " ('file:/Users/koitaroh/Documents/Data/Tweet/ClassifiedTweet_20150709/2015031714_12147_3122.txt',\n",
       "  4,\n",
       "  0.03676470588235294)]"
      ]
     },
     "execution_count": 7,
     "output_type": "execute_result",
     "metadata": {}
    }
   ],
   "source": [
    "doctopic_triples = sorted(doctopic_triples, key=operator.itemgetter(0,1))\n",
    "doctopic_triples[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['file:/Users/koitaroh/Documents/Data/Tweet/ClassifiedTweet_20150709/2015031714_12147_3122.txt',\n",
       " 'file:/Users/koitaroh/Documents/Data/Tweet/ClassifiedTweet_20150709/2015031714_12408_2433.txt',\n",
       " 'file:/Users/koitaroh/Documents/Data/Tweet/ClassifiedTweet_20150709/2015031714_12414_2434.txt',\n",
       " 'file:/Users/koitaroh/Documents/Data/Tweet/ClassifiedTweet_20150709/2015031714_12422_2435.txt',\n",
       " 'file:/Users/koitaroh/Documents/Data/Tweet/ClassifiedTweet_20150709/2015031714_12644_3745.txt',\n",
       " 'file:/Users/koitaroh/Documents/Data/Tweet/ClassifiedTweet_20150709/2015031714_12645_3745.txt',\n",
       " 'file:/Users/koitaroh/Documents/Data/Tweet/ClassifiedTweet_20150709/2015031714_12680_3756.txt',\n",
       " 'file:/Users/koitaroh/Documents/Data/Tweet/ClassifiedTweet_20150709/2015031714_12694_3755.txt',\n",
       " 'file:/Users/koitaroh/Documents/Data/Tweet/ClassifiedTweet_20150709/2015031714_12698_3756.txt',\n",
       " 'file:/Users/koitaroh/Documents/Data/Tweet/ClassifiedTweet_20150709/2015031714_12700_3757.txt']"
      ]
     },
     "execution_count": 8,
     "output_type": "execute_result",
     "metadata": {}
    }
   ],
   "source": [
    "mallet_docnames = sorted(mallet_docnames)\n",
    "mallet_docnames[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1484214"
      ]
     },
     "execution_count": 9,
     "output_type": "execute_result",
     "metadata": {}
    }
   ],
   "source": [
    "num_docs = len(mallet_docnames)\n",
    "num_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29684280"
      ]
     },
     "execution_count": 10,
     "output_type": "execute_result",
     "metadata": {}
    }
   ],
   "source": [
    "len(doctopic_triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 11,
     "output_type": "execute_result",
     "metadata": {}
    }
   ],
   "source": [
    "num_topics = len(doctopic_triples) // len(mallet_docnames)\n",
    "num_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "doctopic = np.zeros((num_docs, num_topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for triple in doctopic_triples:\n",
    "    docname, topic, share = triple\n",
    "    row_num = mallet_docnames.index(docname)\n",
    "    doctopic[row_num, topic] = share"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.03676471,  0.03676471,  0.06617647,  0.03676471,  0.03676471,\n",
       "         0.03676471,  0.05147059,  0.08088235,  0.06617647,  0.03676471,\n",
       "         0.03676471,  0.08088235,  0.05147059,  0.03676471,  0.08088235,\n",
       "         0.03676471,  0.05147059,  0.05147059,  0.03676471,  0.05147059],\n",
       "       [ 0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "         0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "         0.04545455,  0.04545455,  0.06363636,  0.04545455,  0.06363636,\n",
       "         0.04545455,  0.08181818,  0.06363636,  0.04545455,  0.04545455],\n",
       "       [ 0.04807692,  0.06730769,  0.04807692,  0.04807692,  0.04807692,\n",
       "         0.04807692,  0.04807692,  0.04807692,  0.04807692,  0.04807692,\n",
       "         0.04807692,  0.04807692,  0.04807692,  0.04807692,  0.06730769,\n",
       "         0.04807692,  0.04807692,  0.04807692,  0.04807692,  0.04807692],\n",
       "       [ 0.04166667,  0.04166667,  0.09166667,  0.04166667,  0.04166667,\n",
       "         0.075     ,  0.04166667,  0.04166667,  0.04166667,  0.04166667,\n",
       "         0.05833333,  0.05833333,  0.04166667,  0.04166667,  0.04166667,\n",
       "         0.09166667,  0.04166667,  0.04166667,  0.04166667,  0.04166667],\n",
       "       [ 0.04545455,  0.04545455,  0.04545455,  0.05844156,  0.04545455,\n",
       "         0.03246753,  0.17532468,  0.05844156,  0.03246753,  0.03246753,\n",
       "         0.03246753,  0.05844156,  0.03246753,  0.03246753,  0.03246753,\n",
       "         0.03246753,  0.03246753,  0.0974026 ,  0.04545455,  0.03246753]])"
      ]
     },
     "execution_count": 16,
     "output_type": "execute_result",
     "metadata": {}
    }
   ],
   "source": [
    "doctopic[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.04310345,  0.04310345,  0.04310345,  0.06034483,  0.04310345,\n",
       "         0.04310345,  0.04310345,  0.04310345,  0.04310345,  0.06034483,\n",
       "         0.04310345,  0.04310345,  0.07758621,  0.04310345,  0.04310345,\n",
       "         0.06034483,  0.06034483,  0.06034483,  0.04310345,  0.06034483],\n",
       "       [ 0.0462963 ,  0.0462963 ,  0.0462963 ,  0.0462963 ,  0.0462963 ,\n",
       "         0.0462963 ,  0.0462963 ,  0.0462963 ,  0.06481481,  0.0462963 ,\n",
       "         0.0462963 ,  0.06481481,  0.06481481,  0.0462963 ,  0.0462963 ,\n",
       "         0.0462963 ,  0.06481481,  0.0462963 ,  0.0462963 ,  0.0462963 ],\n",
       "       [ 0.04716981,  0.06603774,  0.06603774,  0.04716981,  0.04716981,\n",
       "         0.04716981,  0.04716981,  0.04716981,  0.04716981,  0.04716981,\n",
       "         0.04716981,  0.04716981,  0.04716981,  0.04716981,  0.06603774,\n",
       "         0.04716981,  0.04716981,  0.04716981,  0.04716981,  0.04716981],\n",
       "       [ 0.04385965,  0.04385965,  0.06140351,  0.04385965,  0.06140351,\n",
       "         0.06140351,  0.04385965,  0.06140351,  0.04385965,  0.04385965,\n",
       "         0.04385965,  0.04385965,  0.04385965,  0.04385965,  0.04385965,\n",
       "         0.04385965,  0.06140351,  0.06140351,  0.06140351,  0.04385965],\n",
       "       [ 0.04310345,  0.04310345,  0.06034483,  0.04310345,  0.06034483,\n",
       "         0.04310345,  0.07758621,  0.04310345,  0.04310345,  0.04310345,\n",
       "         0.04310345,  0.04310345,  0.06034483,  0.04310345,  0.07758621,\n",
       "         0.04310345,  0.04310345,  0.04310345,  0.04310345,  0.06034483]])"
      ]
     },
     "execution_count": 14,
     "output_type": "execute_result",
     "metadata": {}
    }
   ],
   "source": [
    "doctopic[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_names = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 11 total tempo / total spatio\n",
    "# Scenario 12 days / total spatio\n",
    "# Scenario 13 hours / total spatio\n",
    "\n",
    "for fn in filenames:\n",
    "    basename = os.path.basename(fn)\n",
    "    name, ext = os.path.splitext(basename)\n",
    "    name = name.rstrip('0123456789')\n",
    "    name = name.rstrip('_')\n",
    "    name = name.rstrip('0123456789')\n",
    "    name = name.rstrip('_')\n",
    "    unit_names.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_names = np.asarray(unit_names)\n",
    "doctopic_orig = doctopic.copy()\n",
    "num_groups = len(set(unit_names))\n",
    "doctopic_grouped = np.zeros((num_groups, num_topics))\n",
    "for i, name in enumerate(sorted(set(unit_names))):\n",
    "    doctopic_grouped[i, :] = np.mean(doctopic[unit_names == name, :], axis=0)\n",
    "doctopic = doctopic_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'doctopic' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5cc080dfe7eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdoctopic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'doctopic' is not defined"
     ]
    }
   ],
   "source": [
    "doctopic[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doctopic.to_csv('/Users/koitaroh/Documents/GitHub/GeoTweetCollector/data/doctopic.csv', encoding='utf-8')\n",
    "np.savetxt(\"/Users/koitaroh/Documents/GitHub/GeoTweetCollector/data/doctopic_twitter_20150709.csv\", doctopic, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "doctopic =  open('/Users/koitaroh/Documents/GitHub/GeoTweetCollector/data/topic-keys-tweet.txt', encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspecting the result\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# CORPUS_PATH_UNSPLIT = os.path.join('data/', 'ClassifiedTweet_sample')\n",
    "CORPUS_PATH_UNSPLIT = os.path.join('/Users/koitaroh/Documents/Data/Tweet/', 'ClassifiedTweet_20150709')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [os.path.join(CORPUS_PATH_UNSPLIT, fn) for fn in sorted(os.listdir(CORPUS_PATH_UNSPLIT))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(input='filename')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm = vectorizer.fit_transform(filenames)  # a sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1484214, 466392)"
      ]
     },
     "execution_count": 9,
     "output_type": "execute_result",
     "metadata": {}
    }
   ],
   "source": [
    "dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "167866384"
      ]
     },
     "execution_count": 10,
     "output_type": "execute_result",
     "metadata": {}
    }
   ],
   "source": [
    "dtm.data.nbytes  # number of bytes dtm takes up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm.toarray().data.nbytes  # number of bytes dtm as array takes up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70, 20)"
      ]
     },
     "execution_count": 27,
     "output_type": "execute_result",
     "metadata": {}
    }
   ],
   "source": [
    "doctopic_orig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11200"
      ]
     },
     "execution_count": 28,
     "output_type": "execute_result",
     "metadata": {}
    }
   ],
   "source": [
    "doctopic_orig.data.nbytes  # number of bytes document-topic shares take up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "novels = sorted(set(unit_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top topics in...\n",
      "2015031714: 1 11 7\n"
     ]
    }
   ],
   "source": [
    "print(\"Top topics in...\")\n",
    "for i in range(len(doctopic)):\n",
    "    top_topics = np.argsort(doctopic[i,:])[::-1][0:3]\n",
    "    top_topics_str = ' '.join(str(t) for t in top_topics)\n",
    "    print(\"{}: {}\".format(novels[i], top_topics_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/koitaroh/Documents/GitHub/GeoTweetCollector/data/topic-keys-tweet.txt', encoding=\"utf-8\") as input:\n",
    "    topic_keys_lines = input.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_words = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in topic_keys_lines:\n",
    "    _, _, words = line.split('\\t')  # tab-separated\n",
    "    words = words.rstrip().split(' ')  # remove the trailing '\\n'\n",
    "    topic_words.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ん',\n",
       " 'ます',\n",
       " '歌',\n",
       " 'ちゃ',\n",
       " 'きえ',\n",
       " '買いもの',\n",
       " '算数',\n",
       " 'なんか',\n",
       " '彫',\n",
       " 'まさに',\n",
       " 'ρ',\n",
       " '所',\n",
       " 'タイヤ',\n",
       " 'o',\n",
       " 'やっと',\n",
       " 'あ',\n",
       " 'パークスビジョン',\n",
       " 'こんな',\n",
       " 'すぎる']"
      ]
     },
     "execution_count": 36,
     "output_type": "execute_result",
     "metadata": {}
    }
   ],
   "source": [
    "topic_words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: ん ます 歌 ちゃ きえ 買いもの 算数 なんか 彫 まさに\n",
      "Topic 1: た お する とき 春 ちゃん lan 仙台 良かっ 終了\n",
      "Topic 2: in 入っ イングリッシュ 食べる ゝ 使い 落ち着く はご バイト こ\n",
      "Topic 3: な って なる じゃあ wi ラップ 解ける 有線 麗らか 違う\n",
      "Topic 4: こと 今 だる 市 itmedia 抜き差し wwwww みんな 小 年\n",
      "Topic 5: か わ 栃木 バッグ スト ブロック ば 宿題 最近 青\n",
      "Topic 6: 笑 入れ 豆腐 公衆 刺す 番 思わず s 回線 やすい\n",
      "Topic 7: ー よ ﾟ 小学生 長浜 まし 中 無線 でぶ ツイート\n",
      "Topic 8: 今日 好き sendai 問題 台場 過ぎ おっ カスタム それだけ 軽く\n",
      "Topic 9: なっ 気 キムチ 本日 成長 狩り 帝釈天 柴又 勿論 爆笑\n",
      "Topic 10: おもろい 笑 fi なかっ ありがと やつ 付近 少し ホイール じん\n",
      "Topic 11: て き じゃ もっ あんまり 狼 流行っ 一 あと 体\n",
      "Topic 12: だ いい ω 受付 店員 きん 娘 σ 東京 い\n",
      "Topic 13: ね ん 人 夢中 進ん 県境 都 やる 眩しい ねぇ\n",
      "Topic 14: 変え 作っ でる mobile free ぼく う 薄着 可愛く 切れ\n",
      "Topic 15: た さん うどん できる ーツイート 難しかっ takuya 年中 群馬 駅\n",
      "Topic 16: けど ️ いき ちゃっ 初 ก 死刑 うらやま めちゃくちゃ 長野\n",
      "Topic 17: ない てか 剣 ︎ てる わら や 今週 うち たのしん\n",
      "Topic 18: なう ありがとう いえ サービス 無料 時 あちこち 永山 乗っ 言う\n",
      "Topic 19: 県 部屋 暑い たら だろ とく 匂い さ いちご ヒョエエエエエエ\n"
     ]
    }
   ],
   "source": [
    "N_WORDS_DISPLAY = 10\n",
    "for t in range(len(topic_words)):\n",
    "    print(\"Topic {}: {}\".format(t, ' '.join(topic_words[t][:N_WORDS_DISPLAY])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}